---
title: "A Simulation Study on the Comparison of Hypothesis Testing Procedures"
author: "Autumn Biggie, Ruben Sowah, Jingjing Li"
date: "4/22/2022"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Report Section  

As we see in part I, to compare two tests we usually have to make assumptions like normal distribution and equal or unequal variances which may be not true for the population. Furthermore, some parameters are inaccessible or unrealistic to obtain at all, which undoubtedly impedes the efficiency and understanding of the statistic tests. On the other hand, what kinds of test methods are better are often ambiguous. An efficient strategy to address these issues is to use simulation study where you can set desirable distribution and parameters such as mean, variance and sample size and the statistic inference process becomes more concrete for learners. Using simulation method, we can easily generate the data which conform with the requirements of the forementioned two tests and compare the efficiency of the two tests under different parameters. For example, how will two test differs when the null hypothesis, $\mu_1 - \mu_2=0$, is false? How will power and Type I Error will change with variance and sample size? 

For simulation study, we intend to generate two data sets of random samples with predetermined parameters including sample size, mean difference and variance using normal distribution function, `rnorm()`via bootstrap approach. Next, in order to compare the means of two samples, under the null hypothesis (H0), $\mu_1 - \mu_2=0$,  and alternative hypothesis (HA) $\mu_1 - \mu_2 !=0$, we will compute t values using two tests with or without equal variance which are described in part I, count the rejection times and get the rejection ratio  which is can be regarded as type I error (when H0 is true) or power (when HA is true). To compare two test methods, we change the variance, sample size and mean difference and get a series of rejection ratios. 

The hypothesis testing procedures in question are both two-sample t tests: the Pooled Variances Test and the Unequal Variances Test. The assumptions of the tests are similar. Both populations are assumed to be Normally distributed with sample sizes $n_1$ and $n_2$ where variances are unknown. The tests diverge in their assumption about the relationship between the variances of each group. The Pooled Variances Test assumes that the populations have equal variance, $\sigma^2$, while the Unequal Variances Test assumes the variances of the populations, $\sigma^2_1$ and $\sigma^2_2$, are unequal.  

We performed a numerical simulation study to compare the two hypothesis testing procedures described above. Simulation studies are appropriate methods of investigation because they allow the researcher to specify the parameters of the population that numerous samples are taken from. This provides the investigator with prior knowledge of the truth or falsehood of the null hypothesis. As many samples are taken from the populations with specified parameters, the rejection rate can be computed as the proportion of times the test rejects the null hypothesis. Since the researcher has prior knowledge of the truth or falsehood of the null hypothesis, the rejection rate can be interpreted as either a Type I Error Rate or Power value. These measures can be compared for each test to assess its reliability under various conditions. Thus, the numerical simulation technique is an effective method to answer the question at hand because it offers the researcher control over specified parameters and can easily produce many samples without vast amounts of time and resources.  

For our simulation study, we varied the true mean difference, sample size, and true variance. The possible values for true mean difference ($\mu_1 - \mu_2$) were -5, -1, 0, 1, and 5. The possible values for sample size were $n_1$ = 10, 30, 70 and $n_2$ = 10, 30, 70. The value of $\sigma^2_2$ was held constant at 1, while $\sigma^2_1$ took on possible values of 1, 4, and 9.  To conduct the study, we created nested for-loops to generate 100 samples for all possible combinations of the above parameters. Both tests were performed on of the generated samples, and alpha and power values were computed. The resulting data set, `rez`, contains the values for the parameters specified, as well as variables `alpha1` and `alpha2`. It is important to note that `alpha1` and `alpha2` do not always refer to the Type I Error Rate, $\alpha$, but only when the null hypothesis $\mu_1 - \mu_2 = 0$ is true. Otherwise, `alpha1` and `alpha2` refer to the power $1 - \beta$ of each test, where `alpha1` corresponds to the **Pooled Variances Test** and `alpha2` corresponds to the **Unequal Variances Test**.  

The results, displayed graphically, reveal clear differences between the performance of the tests under different conditions. We first discuss our observations from Display Group 1:  

When mean difference is -5 or 5 which is far from 0, the power values of both tests are all equal to 1 regardless of variance and sample size. When difference is -1 or 1, the power value is significantly influenced by sample size. When sample size of two samples are equal,the power values of both tests increase with sample size. Under the same sample size, the power values of both tests decrease with rising variance. When var1 (1) is equal to var2 (1), the power values of the tests are equal or very similar, which are less influenced by sample size. In the case when two sample sizes are equal, the power values of the two tests are equal or very close (the blue dots and red dots overlap). When the size of sample 2 is larger than that of sample 1, the power value of test one (with pooled variance) is greater than that of test two (unequal variances test). On the other hand, When the size of sample 2 is smaller than that of sample 1, the power values of test two (with unequal variance) are correspondingly greater than those of test one (pooled test).

The figures in Display Groups 2 and 3 group the variable combinations by differences in value. For example, instead of displaying the values of `var_1` and `var_2`, observations are grouped by the absolute value of the difference between `var_1` and `var_2`. The purpose is to gain insight into the probability of rejection for each test when at different "distances" between means, variances, and sample sizes. This format also improves graph readability.

Both figures in Display Group 2 reveal that Type 1 Error Rate increases as difference in variance increases. However, this error rate, although increasing, remains more stable for the Unequal Variances Test than the Pooled Variances Test. In addition, the power of both tests decreases as variance increases, but only when the difference in means is 1 or -1. When the difference in means is 5 or -5, the power is 1, presumably because the falsehood of the null hypothesis is so apparent that it would be very difficult for both tests to fail to reject. Although the power of both tests decreases as variance increases, as stated previously, the power values remain slightly more constant for the Unequal Variances test, although the difference is minute. The figures in Display Group 3 allow us to examine this difference more clearly. When the absolute mean difference is 1, the power values of both tests decrease as a function on increasing difference in variance, but there are more low power values for the Pooled Variances Test.

Lastly, the the figures in Display Group 3 clearly show the difference in Type I Error rate discussed previously. When the mean absolute difference is 0 (the null hypothesis is true) and the variances are equal, there is not much difference between the Type I Error Rates of the two tests. However, when variances are unequal, the Type I Error rates of the Pooled Variances Test climb much higher than those of the Unequal Variances Test. The Figures in Display Group 3 plot sample size differences in different colors, but because of the interaction between sample size and test type as they relate to power discussed previously, the output is not very interpretable. Identification of this interaction is one of the advantages of the figure in Display Group 1. 

Based on our investigation, we conclude that generally, the Unequal Variances Test is the better procedure. It yields higher power values and lower Type I Error rates in a variety of circumstances. However, in the case where the difference in means is not 0 and the sample size of group 2 is greater than that of group 1, the Pooled Variance Test is the preferred test because it yields higher power values. We hope this report will both add to the information available on these two procedures as well as the credibility of the simulation study as a method for experimentation.  



# Corresponding R Code  

**Objective**: Conduct a simulation study to compare the Type I Error Rate and Power of two hypothesis testing procedures under various conditions. The procedures to be tested include:
  * Pooled Variance Test  
  * Unequal Variance Test  


1) Define variables

  * `var1` and `var2` are the true respective variances for nonsmokers and smokers populations.  
  * `n1` and `n2` are the sample sizes for nonsmokers and smokers populations.  
  * `mu1` and `mu2` are the respective true means for nonsmokers and smokers populations.  
  * `rez` is the dataset generated using simulation combinations (135 in our case).  
  * `yval1` is a set of 9 random numbers based on  sample n1 and yval2 is as set of 9 random numbers based on sample n2.   
  
  

## Assign variables  

The following variables will be used to set conditions for the simulation. We'll be adjusting the values of variance for each group, as well as sample size and mean.  


```{r, echo=FALSE}
#set values of parameters 
var1 <- c(1,4,9) #set variance values for sample 1
var2 <- 1  #set variance values for sample 2

n1 <- c(10,10,10,30,30,30,70,70,70) # set the size of sample 1
n2 <- c(10,30,70,10,30,70,10,30,70) # set the size of sample 2

mu1 <- c(20,20,20,21,25) # set the mean value of sample 1
mu2 <- c(25,21,20,20,20) # set the mean value of sample 2

rez <- data.frame(matrix(ncol=7, nrow = 0)) # set the format of the dataset to be generated. 
colnames(rez) <- c('var_1', 'var_2', 'n_1', 'n_2', 'mu_diff', 'alpha1','alpha2') # Assign the column names to the dataset
```




## Generate Simulation    

The following loop is used to generate random samples from the Normal distribution using all combinations of variance, sample size, and mean defined above. We also compute variables named `alpha1` and `alpha2`. It is important to note that `alpha1` and `alpha2` do not always refer to the Type I Error Rate, $\alpha$, but only when the null hypothesis $\mu_1 - \mu_2 = 0$ is true. Otherwise, `alpha1` and `alpha2` refer to the power $1 - \beta$ of each test, where `alpha1` corresponds to the **Pooled Variance Test** and `alpha2` corresponds to the **Unequal Variances Test**.  


```{r, echo=FALSE}
set.seed(458) # getting reproducible random result

# Attribute vector property to random variables for loop
Tpooled <- vector("numeric",100) #For t test value of pooled method with equal variance 
Tuneq <- vector("numeric",100) # For t test value of the method with unequal variance 
pval1 <-vector("numeric",100) # For p-value of pooled method with equal variance 
pval2 <-vector("numeric",100) # For p-value of the method with unequal variance 

# Generate dataset using nested loops
for (i in 1:3){
  for (j in 1:9){
    for (k in 1:5){
      #Giving initial values of random variables
      test1p_reject <- 0
      test2p_reject <- 0 
      mu_diff <- mu1[k] - mu2[k] #the difference of the means of two samples
      var_1 <- var1[i]
      var_2 <- 1
      n_1 <- n1[j]
      n_2 <- n2[j]
    
       #Using bootstrap to generate 100 samples with identical parameters     to calculate type I error alpha or power 
      for (m in 1:100){

      #generating the data
      yval1 <- rnorm(n1[j], mu1[k], sqrt(var1[i])) 
      yval2 <- rnorm(n2[j], mu2[k], sqrt(var2))
      
      #calculating values to do pooled t-test
      Sp2 <- sqrt(((n_1-1)*var(yval1) + (n_2-1)*var(yval2))/(n_1 + n_2-2))
      Tpooled[m] <- (mean(yval1) - mean(yval2))/(Sp2*sqrt(1/n_1 + 1/n_2))
     
      #calculating p-value corresponding to obtained t value of  pooled t-test
      pval1[m] <- round(2*pt(abs(Tpooled[m]), df=n_1 + n_2-2,lower.tail = F),4)
      
      #counting rejection times of pooled t-test
      if (pval1[m] <= 0.05){
        test1p_reject <- test1p_reject+1
      } 
     
      #calculating values to do unequal variances test
      Tuneq[m] <- (mean(yval1) - mean(yval2))/sqrt(var(yval1)/n_1 + var(yval2)/n_2)
      v <- (var(yval1)/n_1 + var(yval2)/n_2)**2/((var(yval1)/n_1)**2/(n_1 - 1)
          + (var(yval2)/n_2)**2/(n_2 - 1)) #using Satterthwaite's approx
      
        #calculating p-value corresponding to obtained t value of  unequal variances test
      pval2[m] <- round(2*pt(abs(Tuneq[m]), df=v, lower.tail = F),4)
      
      #counting rejection times of unequal variances test
      if (pval2[m] < 0.05){
        test2p_reject <-test2p_reject + 1} 
      
      }
      #calculating alpha values from counted rejections 
      alpha1 <- test1p_reject/100
      alpha2 <- test2p_reject/100

    #combine relevant data to create dataset row
    rez[nrow(rez) + 1,] <- c(var_1, var_2, n_1, n_2, mu_diff,alpha1  ,alpha2)
    
    }
   
  }
}
head(rez)
```



## Graphical Displays  

We use the following graphical displays to represent the data from our simulation study.  

First, new variables are created to allow for better visualizations.

  * `abs_mu_diff` is the absolute difference of the means from each group  
  * `abs_var_diff` is the absolute difference of the variances from each group  
  * `abs_size_diff` is the absolute difference in sample sizes from each group  

These are added to the `rez` data set.  


```{r, echo=FALSE}
rez <- rez %>% mutate(abs_mu_diff = as.factor(abs(mu_diff)), 
                      abs_var_diff = as.factor(abs(var_1 - var_2)),
                      abs_size_diff = as.factor(abs(n_1 - n_2)))
```

### Display Group 1  

This graph shows the probability of rejection for the **Pooled Variances Test** in red and the **Unequal Variances Test** in blue as a function of difference in means, faceted by all possible combinations of `n_1`, `n_2`, and `var_1`. The results are discussed in the report above. 

```{r, echo=FALSE}
# plot alpha value against mu_diff at different conditions by combining different variance and sample size.
rez$n_1 <- as.factor(rez$n_1)
rez$n_2 <- as.factor(rez$n_2)
rez$var_1 <- as.factor(rez$var_1)

n1_names <- c("10" = "n1 = 10", 
                       "30" = "n1 = 30", 
                       "70" = "n1 = 70")
n2_names <- c("10" = "n2 = 10", 
                       "30" = "n2 = 30", 
                       "70" = "n2 = 70")
var1_names <- c("1" = "var1 = 1",
                "4" = "var1 = 4",
                "9" = "var1 = 9")

g  <- ggplot(rez,aes(x=mu_diff)) + 
   geom_point(aes(y=alpha1), color="red") +
    geom_point(aes(y=alpha2), color="blue")  
g +  facet_wrap(n_1+n_2 ~ var_1,
                labeller = labeller(n_1 = n1_names, n_2 = n2_names, var_1 = var1_names)) + 
  theme(strip.text.x = element_text(margin = margin(0, 0, 0, 0))) + 
  labs(title = "Probability of Rejection Vs. True Difference in Means Over All Possible Conditions",
       subtitle = "Pooled Variance Test in red, Unequal Variance Test in blue",
       x = "Difference in Means",
       y = "Probability of Rejection")
```




### Display Group 2  

This graph shows the probability of rejection (`alpha1`) for the **Pooled Variances Test** as a function of the difference in means, faceted by difference in variance. The results are discussed in the report above.  


```{r, warning=FALSE, echo=FALSE}
abs_var_diff_names <- c("0" = "var_diff = 0", 
                       "3" = "var_diff = 3", 
                       "8" = "var_diff = 8")
vardifflabeller <- function(variable, value){return(abs_var_diff_names[value])}


g <- ggplot(rez, aes(x = mu_diff, y = alpha1))
g + geom_point() + 
  stat_smooth(geom = "line", alpha = 0.3, color = "#8856a7", size = 1.5) + 
  ylim(0.0,1.0) + 
  facet_wrap(~abs_var_diff,
             labeller = vardifflabeller) + 
  labs(title = "Pooled Variances Test:",
       subtitle = "a Comparison of Probability of Rejection over Difference in Variance",
       x = "Difference in Means",
       y = "Probability of Rejection")
```




This graph shows the probability of rejection (`alpha2`) for the **Unequal Variances Test** as a function of the difference in means, faceted by difference in variance. The results are discussed in the report above.  


```{r warning=FALSE , echo=FALSE }
g1 <- ggplot(rez, aes(x = mu_diff, y = alpha2))
g1 + geom_point() + 
  stat_smooth(geom = "line", alpha = 0.5, color = "#9ebcda", size = 1.5) + 
  ylim(0.0,1.0) + 
  facet_wrap(~abs_var_diff,
             labeller = vardifflabeller) + 
  labs(title = "Unequal Variances Test:",
       subtitle = "a Comparison of Probability of Rejection over Difference in Variance",
       x = "Difference in Means",
       y = "Probability of Rejection")
```



### Display Group 3  


This graph shows the probability of rejection (`alpha1`) for the **Pooled Variances Test** as a function of the difference in variances, faceted by difference in means. The results are discussed in the report above. Note that the points for `mean_diff = 5` are layered on top of one another, which is why most points in this facet are not visible.  


```{r  warning= FALSE , echo=FALSE}
abs_mu_diff_names <- c("0" = "mean_diff = 0", 
                       "1" = "mean_diff = 1", 
                       "5" = "mean_diff = 5")
mudifflabeller <- function(variable, value){return(abs_mu_diff_names[value])}

g2 <- ggplot(rez, aes(x = abs_var_diff, y = alpha1, color = abs_size_diff, group = abs_size_diff))
g2 + geom_point() + 
  stat_smooth(geom = "line") +
  ylim(0.0,1.0) +
  labs(title = "Pooled Variances Test:",
       subtitle = "a Comparison of Probability of Rejection over Absolute Mean Difference",
       x = "Absolute Difference in Variance",
       y = "Probability of Rejection") +
  scale_color_discrete(name = "Difference in \nSample Size") + 
  facet_wrap(~abs_mu_diff, 
             labeller = mudifflabeller)
```



This graph shows the probability of rejection (`alpha2`) for the **Unequal Variances Test** as a function of the difference in variances, faceted by difference in means. The results are discussed in the report above. Note that the points for `mean_diff = 5` are layered on top of one another, which is why most points in this facet are not visible.  


```{r warning=FALSE , echo=FALSE}
g3 <- ggplot(rez, aes(x = abs_var_diff, y = alpha2, color = abs_size_diff, group = abs_size_diff))
g3 + geom_point() + 
  stat_smooth(geom = "line") +
  ylim(0.0,1.0) +
  labs(title = "Unequal Variances Test:",
       subtitle = "a Comparison of Probability of Rejection over Absolute Mean Difference",
       x = "Absolute Difference in Variance",
       y = "Probability of Rejection") +
  scale_color_discrete(name = "Difference in \nSample Size") + 
  facet_wrap(~abs_mu_diff, 
             labeller = mudifflabeller)
```




### Member Contributions  


### References:
https://www.ime.usp.br/~abe/ICOTS7/Proceedings/PDFs/InvitedPapers/7G2_ERIC.pdf

